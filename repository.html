<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Fabric Repository</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Simple line icons-->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.5.5/css/simple-line-icons.min.css" rel="stylesheet" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <div class="topnav">
            <a href="index.html">Fabric</a></li>
            <a href="index.html">Home</a>
            <a class=active href="#page-top">Repository</a>
            <a href="governance.html">Governance</a>
        </div>
        <!--<section class="content-section bg-primary text-white text-left" id="gov">-->
        <section class="content-section-non-home text-left" id="gov"></section>    
            <div class="container px-4 px-lg-5 text-left">
                <div class="row gx-4 gx-lg-5 justify-content-left">
                    <div class="col-lg-10">
                        <h1>All AI Use Cases</h2>
                        <p class="lead mb-5">
                            The AI use cases are from the private, public, or civil society sectors. In the AI workflows, the dark blue colored shapes represent governance mechanisms and the diamonds are decision points.<p>
                    </div>
                </div>
            </div>
            <div style="text-align: center; margin-bottom: 20px;">
                <input type="text" id="searchInput" placeholder="Search title, sector, or governance..."
                    style="padding: 8px; margin-right: 10px; width: 300px; border-radius: 5px; border: 1px solid #ccc;">
                <select id="sectorFilter" style="padding: 8px; margin-right: 10px;">
                    <option value="">All Sectors</option>
                    <option value="public">Public</option>
                    <option value="private">Private</option>
                    <option value="civil society">Civil Society</option>
                </select>
                <select id="functionalSectorFilter" style="padding: 8px;">
                    <option value="">All Domains</option>
                    <option value="health">Health</option>
                    <option value="technology">Technology</option>
                    <option value="finance">Finance</option>
                    <option value="justice">Justice</option>
                    <option value="human rights">Human Rights</option>
                    <option value="communication">Communication</option>
                    <option value="education">Education</option>
                    <option value="public services">Public Services</option>
                </select>
                <select id="governanceFilter" style="padding: 8px;">
                    <option value="">All Outcome Governance Levels</option>
                    <option value="autonomous ai">Autonomous AI</option>
                    <option value="conditionally autonomous ai">Conditionally Autonomous AI</option>
                    <option value="human-approved ai">Human-Approved AI</option>
                    <option value="human-led with ai-assistance">Human-Led with AI-Assistance</option>
                </select>
            </div>
            <div class="content-section-non-home image-container justify-content-center">
                <div class="image-item">
                    <div class="image-frame">
                        <img src="assets/img/use-cases/credit-lending.png" alt="Image 2"
                            data-title="Credit Lending Classifier"
                            data-sector="Private"
                            data-governance="Human-Approved AI"
                            data-functional="Finance"
                            data-info=" <br>
                            <b>Sector</b>: Private <br><br>
                            <b>Domain</b>: Finance <br><br>
                            <b>Task</b>: The AI system processes loan applications and generates approval recommendations. The recommendation is reviewed by one or more credit officers. The number of human reviewers depends on the size of the loan request, small loans may be reviewed by a single officer, while larger applications require the approval of multiple officers. These officers retain total authority to accept or reject the system recommended application result.
                            <br> <br>
                            <b>Intent</b>: The main intent behind the AI is to support credit officers and increase efficiency in the lending process.
                            <br><br>
                            <b>Risks</b>: There are potential fairness risks if the system embeds bias from historic data. Transparency on how recommendations are generated is critical. Over reliance on the systems output could reduce critical evaluation by officers, especially for borderline applications.
                            <br><br>
                            <b>Outcome Governance Category</b>: Human-Approved AI
                            <br><br>
                            <b>Institutional Governance Examples</b>: the number of analysts reviewing the loan (organization policy), lending rules (regulation), data protection (regulation), consumer protections (regulation)
                            <br><br>
                            <b>Explanation of workflow</b>: <br>
                                <i>Input</i>: A customer's loan application, along with internal customer information and external credit reports are passed to the AI system. <br>
                                <i>Process</i>: The AI system generates an approval recommendation for the application and feature importance scores which show how much each input feature influenced the system’s output. <br>
                                <i>Output</i>: One or more credit officers review the AI's recommendation. The loan will ultimately be approved or denied, depending on the agreement of the credit officers with the AI recommendation.
                                ">
                    </div>
                    <p class="image-title">Credit Lending Classifier</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                        <img src="assets/img/use-cases/metadata.png" alt=""
                            data-title="Metadata Extractor"
                            data-sector="Civil Society"
                            data-governance="Human-Approved AI"
                            data-functional="Human Rights"
                            data-info=" <br>
                            <b>Sector</b>: Civil Society <br><br>
                            <b>Domain</b>: Human Rights <br><br>
                            <b>Task</b>: A user can train an AI model which can extract metadata from text or documents using this AI system. If the user is satisfied with the performance of the outputted AI model, then they can use it.
                            <br> <br>
                            <b>Intent</b>: The main impact of this AI system is to save time for users who cannot afford to scan through hundreds of PDFs or paragraphs. The AI supports efficiency.               <br><br>
                            <b>Risks</b>: The trained AI model could make mistakes when the user uses it.            <br><br>
                            <b>Outcome Governance Category</b>: Human-Approved AI
                            <br><br>
                            <b>Institutional Governance Examples</b>: models should be pulled from a specific commit number (ad-hoc practice), services that use the AI should have a release version (ad-hoc practice), all benchmarks should be saved in a public repository (organization best practice), test sets to assess that performance is maintained (best practice), code implemented is open-sourced (organization policy), services should be covered with unit tests, integration tests, and end-to-end tests (organization policy)
                            <br><br>
                            <b>Explanation of workflow</b>: <br>
                                <i>Input</i>: The user submits samples to the metadata UI, which then extracts the necessary information to train an AI model. <br>
                                <i>Process</i>: The AI generates its intial predictions based on the metadata structure given to the system. <br>
                                <i>Output</i>: The user reviews the newly trained model's predictions and either: accepts the predictions so the trained model is finalized, or rejects the predictions so the trained model is deleted or re-trained with updated data.
                                ">
                    </div>
                    <p class="image-title">Metadata Extractor</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/pdfseg.png" alt=""
                        data-title="PDF Segmentator"
                        data-sector="Civil Society"
                        data-governance="Human-Approved AI"
                        data-functional="Human Rights"
                        data-info=" <br>
                        <b>Sector</b>: Civil Society <br><br>
                        <b>Domain</b>: Human Rights <br><br>
                        <b>Task</b>: A user requires their PDF to be segmented so inputs it to an AI system and then they receive the segmented PDF in a JSON to use.
                        <br> <br>
                        <b>Intent</b>: The AI system supports users in collecting information they need for different tasks.                 <br><br>
                        <b>Risks</b>: The AI could make mistakes when annotating and segmenting the PDF.                 <br><br>
                        <b>Outcome Governance Category</b>: 
                        <br><br>
                        <b>Institutional Governance Examples</b>: models should be pulled from a specific commit number (ad-hoc practice), services that use the AI should have a release version (ad-hoc practice), all benchmarks should be saved in a public repository (organization best practice), test sets to assess that performance is maintained (best practice), code implemented is open-sourced (organization policy), services should be covered with unit tests, integration tests, and end-to-end tests (organization policy)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A user submits a single PDF to a UI. <br>
                            <i>Process</i>: The UI calls on an already-trained AI model 1 to segment the PDF, and then the output is fed as the input to another already-trained AI model 2 which classifies the segmentations. The outputs of the two AI models are a JSON and an annotated PDF. <br>
                            <i>Output</i>: The user checks the output of the PDF. If they are satisfied with the annotated PDF, they can accept it and use the JSON for their task. Otherwise, if they reject the annotated PDF, then the user chooses to not use the JSON.  
                            ">
                    </div>
                    <p class="image-title">PDF Segmentator</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/assessment-tool.png" alt=""
                        data-title="Personalized Feedback Assessor"
                        data-sector="Public"
                        data-governance="Conditionally Autonomous AI"
                        data-functional="Education"
                        data-info=" <br>
                        <b>Sector</b>: Public<br><br>
                        <b>Domain</b>: Education <br><br>
                        <b>Task</b>: Students take practice quizzes to prepare for exams and this AI system provides them with personalized feedback given the students' answers. Teachers can view the students' responses and performance as well as the AI's feedback.
                        <br> <br>
                        <b>Intent</b>: The AI system aims to provide students with personal feedback to speed up their learning process. It also saves teachers time by marking questions for students.                 <br><br>
                        <b>Risks</b>: The AI system can make mistakes which could confuse students.                 <br><br>
                        <b>Outcome Governance Category</b>: Conditionally Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: safety metrics and thresholds chosen (ad-hoc)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A student takes a quiz and the quiz answers are inputted into the AI system. <br>
                            <i>Process</i>: The AI system reviews the answers and generates feedback. The generated feedback responses are checked with safety metrics that relate to certain questions (e.g., is the answer scientifically correct, is harmful language used, and is it aligned with the student's knowledge?). If the score is below a certain threshold, then the feedback is sent to the teacher and the feedback is regenerated.  <br>
                            <i>Output</i>: Once the AI system's feedback responses pass the safety metrics and/or the teacher approves of it, then the answer feedback is provided to the student.
                            ">
                    </div>
                    <p class="image-title">Personalized Feedback Assessor</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/court.png" alt=""
                        data-title="Family Court Support Chatbot"
                        data-sector="Public"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Justice"
                        data-info=" <br>
                        <b>Sector</b>: Public<br><br>
                        <b>Domain</b>: Justice <br><br>
                        <b>Task</b>: This AI system generates responses to questions that users ask in a chat format. The AI pulls from information from the institution's website and can route users to speak with a human representative if required.
                        <br> <br>
                        <b>Intent</b>: The AI system provides immediate and around the clock responses to user questions, as well as reduce the workload of employees.                <br><br>
                        <b>Risks</b>: Incorrect or poor-quality responses could misinform users. Sensitive information shared in the chat may not be completely secure in extreme instances.                 <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: AI cyber-security standard (industry standard), algorithmic transparency standard (industry standard)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A user submits a query to the AI system. <br>
                            <i>Process</i>: The AI system generates a response to the user in the chat. The user views the response and decides if they want to contine the chat. If the user wants to continue the chat, they continue asking questions.   <br>
                            <i>Output</i>: If the user wants to continue the chat but speak to a human agent, they can be routed to an employee who can view their chat history and support them. If the user does not want to continue the chat, they can exit and end the chat.
                            ">
                    </div>
                    <p class="image-title">Family Court Support Chatbot</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/claims.png" alt=""
                        data-title="Insurance Claims Classifier"
                        data-sector="Private"
                        data-governance="Conditionally Autonomous AI"
                        data-functional="Finance"
                        data-info=" <br>
                        <b>Sector</b>: Private<br><br>
                        <b>Domain</b>: Finance <br><br>
                        <b>Task</b>: When customers submit straightforward automotive insurance claims a summary and recommendation is generated by the AI system. 
                        <br> <br>
                        <b>Intent</b>: The AI system is meant to increase productivity and profit by reducing the number of simple claims (if the claim is not overly complex, e.g., a single car part from a commonly insured vehicle) analysts must review.                <br><br>
                        <b>Risks</b>: There is a risk of incorrectly denying legitimate claims if the model is overconfident and mistaken.                  <br><br>
                        <b>Outcome Governance Category</b>: 
                        <br><br>
                        <b>Institutional Governance Examples</b>: validating data provided (organization best practice), denial confidence threshold choice (organization policy), customer can appeal (organization policy), consumer protection practices (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A customer submits an insurance claim. If the claim is deemed a complex claim, then it immediately goes to the claim management system; otherwise, if a simple claim, it is sent to the AI system. <br>
                            <i>Process</i>: If the claim is unconfident, then it will be sent to a claim management system which valides the input data. If the claim is confident (in this case, if it fits a known pattern in historic data), then the AI system processes it and outputs a claim report. If the report recommends denying the claim with low confidence, then it will be sent to the claims management system for review.  <br>
                            <i>Output</i>: If the AI generated report recommended denying the claim with high confidence, then the claim is automatically denied. Otherwise, a claims analyst reviews the claims passed through the claims management system and decides whether to approve each claim. 
                            ">
                    </div>
                    <p class="image-title">Insurance Claims Classifier</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/physio-risk.png" alt=""
                        data-title="Physio-Risk Reporter"
                        data-sector="Private"
                        data-governance="Autonomous AI"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Private<br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: A user (a person over 65 years old) who has limited physical mobility uses this AI system to obtain a risk assessment with respect to their physio-related needs. The report with their risk score is for the user only but they could share it with any carers to request more support if necessary. The users often use the AI system multiple times to check-in on their mobility and physical health overtime.
                        <br> <br>
                        <b>Intent</b>: The report with their risk score is for the user only but they could share it with any carers to request more support if necessary. The users often use the AI system multiple times to check-in on their mobility and physical health overtime.                 <br><br>
                        <b>Risks</b>: The person could injure themselves while doing the exercises or become fatigued.                 <br><br>
                        <b>Outcome Governance Category</b>: Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: user's ability to redo an exercise up to 3 times (organization best practice), a human physiotherapist must be present (organization policy), data protection (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The user completes 12 exercises while being video recorded under the supervision of a human physiotherapist for safety. The video of the user is fed into an already trained AI model 1. <br>
                            <i>Process</i>: The AI model 1 outputs the user's skeleton tracked from the skeleton video using deep learning and computer vision technology. An AI system handler reviews the skeleton video and decides if it is usable or if the user needs to re-complete the exercises in a new video. If the skeleton video is approved of, an already-trained AI model 2, a rule-based system, takes the skeleton video as input. <br>
                            <i>Output</i>: Then, the AI model 2 outputs a PDF report for the user. 
                            ">
                    </div>
                    <p class="image-title">Physio-Risk Reporter</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/writing.png" alt=""
                        data-title="Writing Assistant"
                        data-sector="Private"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Communication"
                        data-info=" <br>
                        <b>Sector</b>: Private<br><br>
                        <b>Domain</b>: Communication <br><br>
                        <b>Task</b>: A user who needs support writing, editing their writing, or brainstorming their ideas for writing can use this AI system.
                        <br> <br>
                        <b>Intent</b>: The AI system is meant to help users better communicate in their writing and to support human creativity, efficiency, and productivity.                <br><br>
                        <b>Risks</b>: The user could become over-reliant on the AI system and begin to de-prioritize their own creativity and skills. The AI system could hallucinate or misunderstand the user's request. <br><br>
                        <b>Outcome Governance Category</b>: 
                        <br><br>
                        <b>Institutional Governance Examples</b>: sensitivity filters from the AI provider (ad-hoc practice)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The user submits a query to the AI system that includes a request for support in writing or editing. <br>
                            <i>Process</i>: The AI system then provides some suggested writing in the document editor. The user can review the AI's output and accept or deny the change. The user can also write at any point.  <br>
                            <i>Output</i>: When the user is satisfied with their writing in the document, the workflow finishes.
                            ">
                    </div>
                    <p class="image-title">Writing Assistant</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/chest-detector.png" alt=""
                        data-title="Chest X-Ray Abnormality Detector"
                        data-sector="Public"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: The AI system reviews a chest X-ray and provides an assessment of the risks flagged and notes where on the image the risks are for a radiologist to review. The AI system prioritizes more risky X-rays so the radiologist sees them first.
                        <br> <br>
                        <b>Intent</b>: The aim of the AI system is to increase the speed and accuracy of chest abnormality screening and prioritize the highest risk patients for review.                <br><br>
                        <b>Risks</b>: The radiologist could become over-reliant on the AI system’s recommendations and exhibit automation bias. The AI system could make mistakes which could lead to a misdiagnosis of a patient or to a risk going undetected.                 <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: prioritization score of X-rays for speeding up radiologist review (organization best practice), post-deployment monitoring for automation bias (organization best practice), data protection (regulation), radiation rules (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The patient's chest X-ray image is sent to the AI system and the radiologist.  <br>
                            <i>Process</i>: The AI system analyzes the X-ray image and makes a new image that overlays the original with points of concern flagged with confidence scores attached. The radiologist sees the original X-ray image first and then the image from the AI system. <br>
                            <i>Output</i>: Based on the level of agreement with the AI system, the radiologist writes their final decision in a report where they make a diagnosis and recommend next steps.
                            ">
                    </div>
                    <p class="image-title">Chest X-Ray Abnormality Detector</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/mental-health.png" alt=""
                        data-title="Mental Health Triage Tool"
                        data-sector="Public"
                        data-governance="Autonomous AI"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public<br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: The AI system generates a preliminary patient report for healthcare providers based on their responses to intake forms.
                        <br> <br>
                        <b>Intent</b>: The AI system is intended to improve intake efficiency and thoroughness. <br><br>
                        <b>Risks</b>: Patients could be mislabelled if they incorrectly answer or skip questions.   <br><br>
                        <b>Outcome Governance Category</b>: Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: lived experience digital group must sign off (organization policy), project and clinical boards must sign off (organization policy), digital clinical safety standards (industry standards), digital clinical safety assurance (industry standards), equality and health impact assessment (regulation), data protection (regulation), medical device compliance (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: Patients respond to questions on mental health intake forms. <br>
                            <i>Process</i>: The AI system will continue to provide the patient with forms until they are completed. It then generates a formulation report on the patient. <br>
                            <i>Output</i>: The formulation report is reviewed by the healthcare provider alongside the patient. 
                            ">
                    </div>
                    <p class="image-title">Mental Health Triage Tool</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/ct-scan.png" alt=""
                        data-title="CT Scan Risk Detector"
                        data-sector="Public"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: When a patient receives a CT scan, an AI system analyses the scan, generates a secondary version with potential concerns flagged, and assigns a prioritization score to speed up its review by a radiologist. 
                        <br> <br>
                        <b>Intent</b>: The AI system is meant to support radiologists in spotting risks in the CT scan and ensure that urgent cases are viewed by radiologists more quickly.          <br><br>
                        <b>Risks</b>:                  <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: prioritization score of scans for speeding up radiologist review (organization best practice), data opt-out scheme (organization policy), data protection (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The patient receives a CT scan which is sent to the AI system and the radiologist(?). <br>
                            <i>Process</i>: The AI system generates a secondary capture of the scan and provides a prioritization score. <br>
                            <i>Output</i>: The radiologist reviews the original and secondary capture CT scans and creates their final report on the patient.
                            ">
                    </div>
                    <p class="image-title">CT Scan Risk Detector</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/note-taker.png" alt=""
                        data-title="Consultation AI Note-Taker"
                        data-sector="Public"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: When a patient has a consultation with their doctor, this AI system generates notes and a referral letter if needed for their doctor to review and use as needed.
                        <br> <br>
                        <b>Intent</b>: This AI system is meant to improve note taking efficiency and thoroughness.                <br><br>
                        <b>Risks</b>: Clinicians could become over reliant on the system for documentation. The AI system could misunderstand something the patient said.               <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: digital clinical safety practices (industry standard), data protection (regulation), equality and health impact assessment (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The patient's consultation with their doctor is recorded and passed to the AI system, with patient approval. <br>
                            <i>Process</i>: The AI system generates consultation notes and predicts if the patient requires a referral. If so, it drafts a referral letter for the clinician.  <br>
                            <i>Output</i>: The clinician reviews the consultation notes, modifies if needed, and then adds the notes to the patient's consultation summary in their chart. If the AI system was correct about a referral, the clinician reviews and can edit the proposed letter before sending. 
                            ">
                    </div>
                    <p class="image-title">Consultation AI Note-Taker</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/carer-ai-updated.png" alt=""
                        data-title="Carer-AI Kit Risk Assessor"
                        data-sector="Public"
                        data-governance="Autonomous AI"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: The AI system is a part of a platform used to validate a carer’s measurements for their patient’s biomarkers (e.g., heart rate, pulse, and blood pressure) and to provide a risk score for the patient.
                        <br> <br>
                        <b>Intent</b>:  The AI system is intended to support in the accurate measurement of a patient’s biomarkers by flagging to the carer when a measurement needs to be retaken. Then, the risk score output is supposed to support the carer in determining the next course of action for the patient.                <br><br>
                        <b>Risks</b>: A risk of the system is that the carer or nurse is not confident in its readings or in their ability to use the devices for biomarker measurements. Another risk is that the medical device has internal bias towards certain groups. Data safety is also a risk is the carer decides that emergency services need to be called then the data sharing must follow regulation and protocol.               <br><br>
                        <b>Outcome Governance Category</b>: Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: digital clinical safety practices (industry standards), data protection (regulation), medical device compliance (regulation), healthcare compliance (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A carer takes biomarker measurements of a patient.  <br>
                            <i>Process</i>: While they are conducting these measurements, an AI system validates that the measurements are accurately taken or flags if measurements need retaking.   <br>
                            <i>Output</i>: The AI system generates a risk score for the carer to review and if needed, the carer take further action for the patient (e.g., request further medical help).  
                            ">
                    </div>
                    <p class="image-title">Carer-AI Kit Risk Assessor</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/smart-triage-updated.png" alt=""
                        data-title="Smart Clinical Triage Tool"
                        data-sector="Public"
                        data-governance="Human-Approved AI"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: Smart Clinical Triage automates the clinical triage process for patients accessing general practice services; it determines clinical urgency, collects structured information from patients, and enables direct booking with the most appropriate provider based on clinical need, capacity, and availability.
                        <br> <br>
                        <b>Intent</b>: This AI system is meant to reduce clinical administrative burden by removing the need for staff-led triage and appointment management, improve patient access by enabling 24/7 digital triage, even outside normal business hours, and standardize care navigation to ensure consistent, equitable triage outcomes across patients.                 <br><br>
                        <b>Risks</b>: A patient could receive an inappropriate classification due to incomplete or misleading patient input.               <br><br>
                        <b>Outcome Governance Category</b>: Human-Approved AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: audit functionality for practitioners (ad-hoc practice), mandatory safety netting and escalation protocols built into every triage path (organization best practice), regular review and updates to triage logic by clinical governance teams (organization policy), data security (industry standard), assessment for services procured (industry standard), digital clinical safety (industry standard), data protection (regulation), medical devices compliance (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: Patients initiate a consultation by completing an online form via their doctor’s practice website. The system may also draw upon pre-existing clinical records.  <br>
                            <i>Process</i>: The AI determines whether the case is an emergency. If so, the patient is immediately redirected to emergency services. For non-emergency cases, the AI dynamically asks follow-up clinical questions, adapting the pathway based on patient responses using a structured rules-based model. The triage concludes with the classification of the clinical problem and a recommended next step.   <br>
                            <i>Output</i>: The system offers the patient appropriate appointment slots with the relevant care provider, which could include doctors, nurses, clinical pharmacists, or community services, and enables direct booking or messaging without needing reception staff.
                            ">
                    </div>
                    <p class="image-title">Smart Clinical Triage Tool</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/local-gov-chatbot-updated.png" alt=""
                        data-title="Local Government Chatbot"
                        data-sector="Public"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Public Services"
                        data-info=" <br>
                        <b>Sector</b>: Public <br><br>
                        <b>Domain</b>: Public Services <br><br>
                        <b>Task</b>: This AI system is a chatbot available within the institution’s website that can answer non-sensitive questions from users.
                        <br> <br>
                        <b>Intent</b>: To provide efficient and around the clock answers to users and reduce the administrative burden of responding to simple, non-personal, questions.                 <br><br>
                        <b>Risks</b>: Despite warning statements, users could still submit personally identifying information which the system is not intended to handle. Any incorrect or misinterpreted responses could lead to misinformed users. <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: internal digital board (organization policy), data privacy (organization policy), data protection (regulation), accessibility requirements (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A user, who is instructed to not provide any personally identifying information, submits a query to the AI system.  <br>
                            <i>Process</i>: The AI system will decide if the query topic is sensitive in nature. If the question is sensitive, then the system will output a pre-defined response. If the topic is not sensitive, then the system will give a generated response based on the website content knowledge base.  <br>
                            <i>Output</i>: The user can then decide if they would like to continue asking questions in the chat, end the conversation, or receive additional help by being directed to a human agent. 
                            ">
                    </div>
                    <p class="image-title">Local Government Chatbot</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/llm-param-optimizer-updated.png" alt=""
                        data-title="Hyperparameter Optimizer for LLM and RAG Systems"
                        data-sector="Private"
                        data-governance="Human-Approved AI"
                        data-functional="Technology"
                        data-info=" <br>
                        <b>Sector</b>: Private <br><br>
                        <b>Domain</b>: Technology <br><br>
                        <b>Task</b>: This AI system helps customers determine optimal configurations for Retrieval Augmented Generation (RAG) pipelines by conducting multi-objective hyperparameter optimization. The system evaluates trade-offs between safety (minimizing hallucinations), alignment (helpfulness), cost, carbon and latency to identify the best combinations of parameters. The system provides users with a Pareto-optimal frontier of configurations, allowing them to select the configuration that best suits their specific requirements and constraints. 
                        <br> <br>
                        <b>Intent</b>: This AI system ideally enables organizations to deploy more efficient and effective RAG solutions by reducing costs and latency through identifying more efficient configurations while simultaneously improving safety by minimizing hallucination risks in generated responses. It intends to enhance alignment to ensure responses are helpful and relevant to user needs and offers quantifiable benchmarks that can be used for regulatory compliance considerations. The AI system aims to allow for informed decision-making based on multiple competing objectives, helping organizations balance performance with resource constraints.                 
                        <br><br>
                        <b>Risks</b>: Users could become over-reliant on AI system outputs and fail to consider factors outside of those that the system uses to calculate its output. The system’s recommendations are based on test datasets which may not fully represent real-world usage patterns, potentially leading to suboptimal configurations in production environments. 
                        <br><br>
                        <b>Outcome Governance Category</b>: Human-Approved AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: transparent documentation of the optimization methodologies used (organization best practice), regular benchmarking against established datasets to ensure reliability of recommendations (organization best practice), users retain decision-making authority over final configuration choices (organization best practice), the implementation of safeguards to prevent unsafe configurations from being deployed (organization best practice) 
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A user submits their inputs to the AI system, including document data, model options, and example queries relevant to their use case.  <br>
                            <i>Process</i>: The AI system runs an optimization process that tests various configurations, evaluating each one on five metrics: safety (how factual the responses are), alignment (how helpful the responses are), cost (expense per million queries), carbon (CO2 emissions) and latency (response time). The system produces a visualization showing the trade-offs between different configurations to the user.   <br>
                            <i>Output</i>: Users can examine the results from the AI system and select the configuration that best balances their priorities, enabling informed decision-making without requiring technical expertise in RAG system configuration. If they are not satisfied with the configurations presented, they can modify their input and start the optimization process again. 
                            ">
                    </div>
                    <p class="image-title">Hyperparameter Optimizer for LLM and RAG Systems</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/origin-trail-dkg-updated.png" alt=""
                        data-title="OriginTrail Decentralized Knowledge Graph"
                        data-sector="Private"
                        data-governance="Conditionally Autonomous AI"
                        data-functional="Technology"
                        data-info=" <br>
                        <b>Sector</b>: Private <br><br>
                        <b>Domain</b>: Technology <br><br>
                        <b>Task</b>: OriginTrail Decentralized Knowledge Graph (DKG) enables people and agents to turn data into verifiable, structured, and interconnected knowledge. Published knowledge, whether public or private, remains verifiable at the source and usable by both humans and agents. 
                        <br> <br>
                        <b>Intent</b>: OriginTrail’s mission is to bring together structured and connected knowledge from symbolic AI (DKG) with the creativity of GenAI, creating a resilient decentralized AI infrastructure where data is always traceable to its origin.                 
                        <br><br>
                        <b>Risks</b>: A risk of the system is that the agents could make mistakes while they create the knowledge graph or the approval agent could miss a mistake when checking the DKG. The human approving of the agents’ DKG could be over-reliant and miss an error.
                        <br><br>
                        <b>Outcome Governance Category</b>: Conditionally Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: user and/or community led verification of knowledge graph (ad-hoc practice)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A user submits data (e.g., PDFs) to the AI system.  <br>
                            <i>Process</i>: The AI system is made up of agent(s) that transform input data into a knowledge graph and publish it on the DKG. These agents can have different roles (e.g., coordinator or expert of a certain ontology). Given the data and the task required of them, they build up the knowledge graph draft with nodes and connections. The agent(s) output this knowledge graph draft which is ready to be published on the DKG and connected with other knowledge there.  <br>
                            <i>Output</i>: A review gateway can be implemented where human or agent can approve or adjust the knowledge graph draft before it is published to the DKG. Once published, the knowledge graph draft becomes part of the DKG, gaining cryptographic source verifiability (via blockchain) and becoming interlinked with other knowledge on the network. If published publicly, it can be discovered and used by humans or agents through the DKG. 
                            ">
                    </div>
                    <p class="image-title">OriginTrail Decentralized Knowledge Graph</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/automated-image-protocol-selector.png" alt=""
                        data-title="Automated Imaging Protocol Selector"
                        data-sector="Private"
                        data-governance="Human-Led with AI-Assistance"
                        data-functional="Health"
                        data-info=" <br>
                        <b>Sector</b>: Private <br><br>
                        <b>Domain</b>: Health <br><br>
                        <b>Task</b>: The AI system automatically selects radiology exposure parameters to support the radiographer based on information about the patient.  
                        <br> <br>
                        <b>Intent</b>: The intent of the AI system is to select the optimal radiology exposure parameters to enable diagnostic reading while limiting the patient exposure to harmful ionizing radiation. Too little X-ray dose reduces the radiologist’s ability to detect clinical findings due to a lack of image contrast. This could lead to false negative findings, or the radiographer deciding to make a retake, which causes a loss of resources and places the patient on the X-ray table for longer than necessary, while over-exposing the patient to harmful ionizing radiation.                  
                        <br><br>
                        <b>Risks</b>: The patient could receive excessive ionizing radiation which may cause cell damage and potentially lead to cancer and at the most extreme, death. 
                        <br><br>
                        <b>Outcome Governance Category</b>: Human-Led with AI-Assistance
                        <br><br>
                        <b>Institutional Governance Examples</b>: in-house standard operating procedures like equipment settings and staff competency protocols (organization best practices), mammography techniques like automatic exposure control and acceptable dose levels (industry best practice), patient communication and documentation (industry best practice), basic safety standards for radiation protection (regulation), personnel qualification (regulation), equipment related testing, quality control, and monitoring (regulation), informed consent (regulation)
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: The patient’s data and doctor’s intake information go into the AI system.  <br>
                            <i>Process</i>: The rule-based AI system uses input data from the radiographer and interconnected systems such as the Electronic Patient Record System and Radiology Information System (such as age, weight, gender, height, body part) and pre-programmed thresholds set by the manufacturer and potentially adjusted by the manufacturers installation engineer, the health institutions medical physicists or quality control staff based on the recommended exposure parameters as defined in laws and guidance.   <br>
                            <i>Output</i>: The radiographer reviews the suggested exposure parameters and can accept or modify them before acquiring the X-ray image. 
                            ">
                    </div>
                    <p class="image-title">Automated Imaging Protocol Selector</p>
                </div>
                <div class="image-item">
                    <div class="image-frame">
                    <img src="assets/img/use-cases/image-blurring-tool.png" alt=""
                        data-title="Image Blurring Tool"
                        data-sector="Public"
                        data-governance="Autonomous AI"
                        data-functional="Public Services"
                        data-info=" <br>
                        <b>Sector</b>: Private <br><br>
                        <b>Domain</b>: Public Services <br><br>
                        <b>Task</b>: The AI system is used to blur out any people shown in an image of a public area to prioritize people’s privacy. Then, the blurred images are used for many different tasks (e.g., images can flag when certain things in public areas need attention by the government like a trash bin being out of place).   
                        <br> <br>
                        <b>Intent</b>: To maintain the privacy of individuals captured on cameras used to support public services. Many public service use cases are built on top of the collection of these blurred images.                   
                        <br><br>
                        <b>Risks</b>: The AI system could be bias towards certain groups of people. The AI system could mistake something that isn’t a person as a person and blur an object that should be clear. The AI system could miss blurring a person if it does not recognize them as a human.  
                        <br><br>
                        <b>Outcome Governance Category</b>: Autonomous AI
                        <br><br>
                        <b>Institutional Governance Examples</b>: ethics impact assessment (organizational best practice), bias assessment (organizational best practice), algorithm register for transparency (regulation) 
                        <br><br>
                        <b>Explanation of workflow</b>: <br>
                            <i>Input</i>: A vehicle takes photos of public areas and the photos are given to an AI system. Once sent to the AI system, the original photos with people are not saved anywhere.   <br>
                            <i>Process</i>: The AI system uses computer vision technology to locate people in the photos and then overlays the people identified with a blurring effect.    <br>
                            <i>Output</i>: The AI outputs the image with people blurred and it is then used as a image of a public space for a public servant to use for other tasks. If the public servant notices that the AI system did not blur out one or more people, then they can flag that issue and the AI system will be given more data and retrained at a later stage.  
                            ">
                    </div>
                    <p class="image-title">Image Blurring Tool</p>
                </div>
            </div>
            <!-- Modal Structure -->
            <div id="infoModal" class="modal">
                <div class="modal-content">
                    <span class="close">&times;</span>
                    <img id="modalImage" src="" alt="">
                    <div class="modal-title" id="modalTitle"></div>
                    <div class="modal-description" id="modalDescription"></div>
                </div>
            </div>
        </section>
        <footer>
            <div class="copyright_container text-center">
                <p>&copy; 2025 Fabric Repository</p>
            </div>
        </footer>
        <!-- Scroll to Top Button-->
        <a class="scroll-to-top rounded" href="#page-top"><i class="fas fa-angle-up"></i></a>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>